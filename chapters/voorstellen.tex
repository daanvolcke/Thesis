\chapter{Voorstellen voor verdere optimalisaties}

\todo[inline]{Inleiding?}

\section{Bulk inserts bij databankinteracties}

De databankinteracties werden uitvoerig besproken in de \cref{werking,profiling,impact-db}.
Een verdere optimalisatie die zeker nog gedaan moet worden,
is het gebruik van bulk inserts bij het wegschrijven van de opgehaalde gegevens in de databank.

In \cref{impact-db} hebben we gezien dat het wegschrijven van de gegevens in de databank een gigantische impact heeft op de uitvoeringstijd,
dat alleen maar toeneemt met het aantal te bevragen toestellen.
Het bevragen van slechts 100 toestellen ging zonder het wegschrijven van de resultaten maar liefst 61 keer sneller.
Sterker nog, de impact van de databankinteracties is zodanig groot dat de performantiewinst van de andere optimalisaties bijna teniet gedaan wordt.
De nieuwe versie was bij 100 toestellen slechts 5\% sneller dan de oude, maar ruim zeven keer sneller als de gegevens niet weggeschreven moesten worden.
Het implementeren van bulk inserts is dus absoluut cruciaal om de \nwmretriever{} succesvol te kunnen inzetten op grote schaal.

Het implementeren van bulk requests hoeft zelfs niet zoveel werk en tijd te vereisen.
Een eenvoudige implementatie houdt een globale tabel bij in het geheugen die de resultaten tijdelijk bijhoudt.
De InsertResultRow-methode wordt dan aangepast om de gegevens in die tabel in het geheugen weg te schrijven in plaats van rechtstreeks in de databank.
Eenmaal een bepaalde hoeveelheid gegevens in de databank weggeschreven zijn kan de InsertResultRow-methode dan beslissen om al die data
in een keer weg te schrijven in de databank met dus een bulk insert.

Het nadeel van deze manier van werken is dat het wegschrijven van de resultaten in dezelfde thread gebeurt als het bevragen van een toestel.
Als de gegevens weggeschreven worden zal die thread en het bevragen van dat ene toestel een stuk langer duren dan anders.
Een andere manier van werken die wat meer arbeidsintensief is en meer tijd vraagt werd ook al voorgesteld in \cref{impact-db}.

Daarbij wordt de verantwoordelijkheid van het wegschrijven van de resultaten in de databank overgeheveld naar een aparte thread.
Deze zal dan instaan om, wanneer de tabel in het geheugen een bepaalde hoeveelheid gegevens bevat de gegevens weg te schrijven naar de databank,
zonder de andere retrieverthreads daarbij te storen.
De InsertResultRow-methode is dan enkel nog verantwoordelijk voor het wegschrijven van de gegevens in de tabel in het geheugen.

Men kan dan nagaan of het beter is om periodiek gegevens weg te schrijven tijdens het bevragen van de toestellen,
of om de gegevens ineens weg te schrijven na het bevragen van de toestellen, of naar het einde toe als het aantal retrieverthreads daalt.

De gegevens hoeven zelfs niet weggeschreven te worden in een databank.
Men kan verschillende implementaties voorzien die de tabel in het geheugen op verschillende manieren verwerkt.
Men kan er bijvoorbeeld ook voor kiezen om de gegevens naar een bestand weg te schrijven of een combinatie.
Het grote voordeel is dat het opslaan van de gegevens los staat van het opvragen ervan, dus dat de snelheidsimpact ervan beperkt blijft.


\section{Intelligent threadbeheer}

Het beheer van threads en het CPU-gebruik werden besproken in \cref{werking,cpu-gebruik}.

Er was al gebleken dat het beheer van de threads in de originele versie van de \nwmretriever{} beter kon.
Het probleem was dat er gewacht werd tot de eerst gestarte thread klaar was alvorens nieuwe threads aan te maken voor het bevragen van toestellen.
Dat leidde tot korte periodes waarbij het aantal retrieverthreads zakte en er geen neiuwe threads werden aangemaakt.
Maar zoals gezegd werd dit probleem reeds aangepakt door NetwerkMining tijdens de masterproef en
wordt er gebruik gemaakt van een andere implementatie in de nieuwe versies van de \nwmretriever{}.

We hebben ook gezien dat het aantal threads vaststond op 50 threads.
Een betere manier van werken zou het aantal threads dynamisch bepalen aan de hand van de beschikbare resources (zowel CPU-, RAM- als bandbreedtegebruik).
Uit de tests in \cref{benchmarks-geheugengebruik,benchmarks-bandbreedte} bleek echter dat het geheugen- en bandbreedtegebruik beperkt blijft
en er dus meer dan genoeg reserve over is voor die resources.
Het aantal threads zou dus voornamelijk afhangen van het CPU-gebruik.

Op de server van de Virtual Wall was er nog een overschot aan CPU-rekenkracht,
maar het omgekeerde is ook mogelijk.
Bij een tekort aan rekenkracht heeft het geen zin om meer threads aan te maken en kan het mogelijk zelfs een averechts effect hebben.

Het dynamisch beheren van threads is een algemeen probleem waar veel softwareprojecten mee geconfronteerd worden.
Gelukkig betekent dit ook dat er reeds vele oplossingen voor zijn ontwikkeld die vrij te gebruiken zijn.
Het .NET-platform biedt hiervoor een eigen oplossing onder de vorm van de \gls{tpl} sinds versie 4 van het .NET-framework.

Het doel van \gls{tpl} is om het ontwikkelaars gemakkelijker te maken om gebruik te maken van parallelisme en multithreading in applicaties.
\Gls{tpl} schaalt dynamisch de mate van parallelisme om zo efficiÃ«nt mogelijk gebruik te maken van alle processorcores die beschikbaar zijn.
\Gls{tpl} verzorgt ook het inplannen van threads in een \textit{thread pool}, ondersteuning voor het annuleren van threads, toestandbeheer en
andere low-level details\cite{msdn-tpl}.
